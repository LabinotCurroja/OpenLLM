# Qwen3-4B Inference Requirements
# For Apple Silicon (MPS)

torch>=2.0.0
transformers>=4.40.0
accelerate>=0.27.0
sentencepiece
protobuf

# TUI dependencies
textual>=0.47.0
rich>=13.0.0

# API server dependencies
flask>=3.0.0
flask-cors>=4.0.0

# Optional: For CUDA INT8 quantization (not needed on Mac)
# bitsandbytes>=0.41.0
